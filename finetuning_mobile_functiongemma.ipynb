{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mariahelenass/models-edge-services/blob/main/finetuning_mobile_functiongemma.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayKaGux9M4G2",
        "outputId": "e29a9d92-acfe-48f7-8999-dbe7221fc612"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m518.9/518.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\n",
            "Dependencies installed!\n"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers==4.57.3 datasets accelerate evaluate trl==0.26.2 protobuf sentencepiece\n",
        "!pip install -q huggingface_hub tensorboard\n",
        "\n",
        "print(\"\\nDependencies installed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "1ad6015ad9c843fc9c76919fce7e4e21",
            "9763c95477274ef4a488552d72ba2ad0",
            "3ee6bf973ebf4955b4f0d6bcc28fe2f3",
            "149baae01053444b8a969666e5f2004e",
            "8c71c58ad55c446790b9e22557c525da",
            "ae94d90612e141e7b4053220b0a51d0c",
            "e55303d7f4444e4891d648870d32a062",
            "6b98148c03654916b679a19d8f277227",
            "17dd54eb21494c04a6d0d1ec91fd2b6c",
            "2b2b7509804b44c8a830699204604ad8",
            "027e77c8d6ca4f86b532576ec42ca213",
            "13b4c184003342db903106fe565b49ea",
            "e41dd42ab12b466ebc8e170dbf9c74fd",
            "464a87dfa6654fcb87f706d454427e6b",
            "da7ef893b0a4499ea537749e3931ff12",
            "99e2c6df97dc4550a1dcb5ad21a7bd7f",
            "30c2149b66a04e7eaf5447c7ff9d70e0",
            "49b330a5b3af4bb7a67c0b751d293873",
            "19720abab86948b492ae928687ddc1c3",
            "764aeb22f0804c608a86672e685baf3c"
          ]
        },
        "id": "c7_gXF1hM7Q6",
        "outputId": "0de5de58-ca24-4c00-fdd6-fa4b4bfa105e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1ad6015ad9c843fc9c76919fce7e4e21",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOqJncVfNxcz"
      },
      "source": [
        "### Preparando tools e dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VfII1ufNACS",
        "outputId": "41e771e7-ec2f-4fbb-955e-18e0e2914480"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tools defined:\n",
            "   - pagamento: Tool de pagamento...\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from datasets import Dataset\n",
        "from transformers.utils import get_json_schema\n",
        "\n",
        "\n",
        "def pagamento(valor: float, metodo_pagamento: str):\n",
        "    \"\"\"\n",
        "    Tool de pagamento\n",
        "\n",
        "    Args:\n",
        "        valor: valor numérico da transação em reais\n",
        "        metodo_pagamento: método utilizado para pagamento (ex: \"pix\", \"debito\", \"credito\")\n",
        "\n",
        "    Returns:\n",
        "        dict: Dicionário contendo mensagem de confirmação e dados de pagamento\n",
        "    \"\"\"\n",
        "    return {\n",
        "        \"mensagem\": \"Pagamento realizado com sucesso\",\n",
        "        \"dados\": {\n",
        "            \"valor\": valor,\n",
        "            \"metodo_pagamento\": metodo_pagamento\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "TOOLS = [\n",
        "    get_json_schema(pagamento)\n",
        "]\n",
        "\n",
        "print(\"Tools defined:\")\n",
        "for tool in TOOLS:\n",
        "    print(f\"   - {tool['function']['name']}: {tool['function']['description'][:50]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415,
          "referenced_widgets": [
            "1b0cd5ec20eb447ba662e3d22129bb10",
            "411f330a40c84998aea85ac5666c6f5d",
            "d434bbf72e6e41b69002b407bedc8970",
            "c05af332ad7c418283a2ba981e8e32f1",
            "795d660d5443405bb9641afe0cf062a0",
            "680483b488a04cc5bee09597141c58d3",
            "6f6967085daf43e9a656c00f4454d7c0",
            "5ea6071f01164d85917c4ae6b97dc600",
            "03b87bd25222489b8680d82d321ecf08",
            "cc1b9010c6cf4e4ebe2036743449ee25",
            "daad7ba5f3044db18c0e18c002f6e726"
          ]
        },
        "id": "k0Ey_TlfQfK2",
        "outputId": "178262e0-7560-403d-d502-3de4e3fc920f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 1000 raw examples\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1b0cd5ec20eb447ba662e3d22129bb10",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Dataset prepared:\n",
            "   Train: 800 examples\n",
            "   Test:  200 examples\n",
            "\n",
            "============================================================\n",
            "Sample training example:\n",
            "============================================================\n",
            "developer\n",
            "You are a model that can do function calling with the following functions\n",
            "declaration:pagamento{description:Realiza um pagamento via Pix,parameters:{properties:{valor:{description:Valor do pagamento em reais,type:NUMBER},metodo_pagamento:{description:Metodo de pagamento,type:STRING}},required:[valor,metodo_pagamento],type:OBJECT}}\n",
            "\n",
            "user\n",
            "pix no valor de 302\n",
            "\n",
            "model\n",
            "call:pagamento{valor:302.0,metodo_pagamento:pix}\n",
            "...\n"
          ]
        }
      ],
      "source": [
        "# STEP: Convert dataset to Google FunctionGemma format\n",
        "\n",
        "import json\n",
        "from datasets import Dataset\n",
        "\n",
        "# FunctionGemma special tokens\n",
        "START_TURN = \"\"\n",
        "END_TURN = \"\"\n",
        "START_DECL = \"\"\n",
        "END_DECL = \"\"\n",
        "START_CALL = \"\"\n",
        "END_CALL = \"\"\n",
        "ESCAPE = \"\"\n",
        "\n",
        "# Function declaration in Google format (PIX payment)\n",
        "FUNCTION_DECLARATIONS = f\"\"\"{START_DECL}declaration:pagamento{{description:{ESCAPE}Realiza um pagamento via Pix{ESCAPE},parameters:{{properties:{{valor:{{description:{ESCAPE}Valor do pagamento em reais{ESCAPE},type:{ESCAPE}NUMBER{ESCAPE}}},metodo_pagamento:{{description:{ESCAPE}Metodo de pagamento{ESCAPE},type:{ESCAPE}STRING{ESCAPE}}}}},required:[{ESCAPE}valor{ESCAPE},{ESCAPE}metodo_pagamento{ESCAPE}],type:{ESCAPE}OBJECT{ESCAPE}}}}}{END_DECL}\"\"\"\n",
        "\n",
        "SYSTEM_PROMPT = f\"\"\"{START_TURN}developer\n",
        "You are a model that can do function calling with the following functions\n",
        "{FUNCTION_DECLARATIONS}\n",
        "{END_TURN}\n",
        "\"\"\"\n",
        "\n",
        "def create_training_example(sample):\n",
        "    \"\"\"\n",
        "    Creates training example in exact Google FunctionGemma format.\n",
        "\n",
        "    Input:\n",
        "    {\n",
        "      \"user_content\": \"manda 10 no pix\",\n",
        "      \"tool_name\": \"pagamento\",\n",
        "      \"tool_arguments\": \"{\\\"valor\\\":10,\\\"metodo_pagamento\\\":\\\"pix\\\"}\"\n",
        "    }\n",
        "\n",
        "    Output text for training:\n",
        "\n",
        "    developer\n",
        "    You are a model...\n",
        "\n",
        "    user\n",
        "    manda 10 no pix\n",
        "\n",
        "    model\n",
        "    call:pagamento{valor:10,metodo_pagamento:pix}\n",
        "    \"\"\"\n",
        "    user_content = sample[\"user_content\"]\n",
        "    tool_name = sample[\"tool_name\"]\n",
        "    tool_args = json.loads(sample[\"tool_arguments\"])\n",
        "\n",
        "    # Build prompt (input)\n",
        "    prompt = f\"\"\"{SYSTEM_PROMPT}{START_TURN}user\n",
        "{user_content}\n",
        "{END_TURN}\n",
        "{START_TURN}model\n",
        "\"\"\"\n",
        "\n",
        "    # IMPORTANT:\n",
        "    # Google format does NOT quote strings inside call\n",
        "    params_str = \",\".join(\n",
        "        [f\"{k}:{v}\" for k, v in tool_args.items()]\n",
        "    )\n",
        "\n",
        "    completion = f\"{START_CALL}call:{tool_name}{{{params_str}}}{END_CALL}\"\n",
        "\n",
        "    return {\"text\": prompt + completion}\n",
        "\n",
        "\n",
        "# Load and convert dataset\n",
        "raw_data = []\n",
        "with open(\"machine_actions.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        raw_data.append(json.loads(line.strip()))\n",
        "\n",
        "print(f\"Loaded {len(raw_data)} raw examples\")\n",
        "\n",
        "dataset = Dataset.from_list(raw_data)\n",
        "dataset = dataset.map(create_training_example, remove_columns=dataset.features)\n",
        "\n",
        "# Split into train/test (80% / 20%)\n",
        "dataset = dataset.train_test_split(test_size=0.2, shuffle=True, seed=42)\n",
        "\n",
        "print(\"\\nDataset prepared:\")\n",
        "print(f\"   Train: {len(dataset['train'])} examples\")\n",
        "print(f\"   Test:  {len(dataset['test'])} examples\")\n",
        "\n",
        "# Show sample\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"Sample training example:\")\n",
        "print(\"=\"*60)\n",
        "print(dataset['train'][0]['text'][:800])\n",
        "print(\"...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433,
          "referenced_widgets": [
            "ecc716276773474e9a1d3d068e84817b",
            "84b3788290fa419faa2e335d624fbacc",
            "48780af990ad4e1186f48a66e48d0f1e",
            "5af8a77e8b0e496caeee8d1ad5d7aab8",
            "dea77ef7dd4e45debafefd42f2324861",
            "bf04a8e14689409ebc02590d4e12d01f",
            "9a4960c1200e4672a5e9f6787b7d757a",
            "62ae83a9e242413b939dea7613f8b3e6",
            "e8e8e9ea33bc4265a74e03be240610f5",
            "281c70b4c75f458181d8501454913f78",
            "bc4e3dd6005c4f2d9c9e8d7b69c09267",
            "20402aaf49b74f7eb2fdb7416e23943a",
            "def58d45593346b2bb8304f276c83d92",
            "36049c043b734c25b2486bb5a9ee20fb",
            "d0d99f305ddc4220b339fb2c6da3cf7c",
            "2018e68d720d4e9c90d4c8f9dd4a5933",
            "df1e3cd13c2844f98c9344431b433bf3",
            "1e2a6b7e7f46485c93028d40a01f0f5a",
            "324f89717fd64dc8a93ded0816f0898c",
            "19913eac9ad54f00925117c5835baf2e",
            "4db87d3095a049e4b0698556b1b1db39",
            "672f89aa49ac4ad493e281e8751c8b26",
            "dac398efc94a46f9a5e875d2dc3ccf73",
            "7f47468c20a140aab17814cb77186771",
            "d90db245ee3549b1a27bd6f5e8f5ea2e",
            "219452d17b1f45b9a10548d0d297decf",
            "fb6950235d8944e99a64c7fcdfa36766",
            "7f722a041d6742289629e65d8bc4f0cc",
            "059a183c143c40ca9b28f8f3d4f97744",
            "1a0734b56eb24c78b88d82db8defabae",
            "bd4c8527a63f4be1bf49267a2d55bdbb",
            "10c330e0ad3d45148d307e69e9bc4e63",
            "9af3aa71371948869de4681b379f0e82",
            "efd8dfd28c504176a552fd66ea78aae7",
            "053e4b5dcdd94828b5a5a84d0fc3c8bd",
            "6c44d29de1c3415d9e81694936143899",
            "5850cedace0d4d1b9028569701245bfc",
            "f8f9c4a0672548e88a8ab12429a24ce3",
            "a2833334215d4800a652803ab273eee1",
            "f546fa34667146dc85f09c0b50370bfb",
            "440943a8936141df92324a78b1105caa",
            "af39ecf261114c9d85c1bcd557ffee4c",
            "5cc42f6d241b43b684b44627dab16e4e",
            "dcc4e05d649a4bebbce4573eaa46f76d",
            "6e156bdb62af4a2780bee18d670efad3",
            "91ef5da8a5b54865b728315216b666b1",
            "bbe495b16b10497091fb0140d4c8f5c2",
            "31b98fdc09c74ad7882d5d373877367c",
            "66318b389ebe4f56870c5eaf652add80",
            "d851a0c00f5543a4a11f9eda8144d0d7",
            "6511dbb149604050bfeace11a5d846e7",
            "dd424ba176404765b7aecad177b8bef9",
            "57fa30e6ec7a48fe9becc8052b6e06be",
            "9c17edf86e684506b986e96930e54fb8",
            "ea23740477dc4cb5867f081bccfa99b1",
            "1751a001d19b49d6b256e3f656425803",
            "a1ec264ad58a4b108e768ff5a0e81f49",
            "f0a16903169348ff895e005595e3c8b1",
            "54d8c638bd5c4b15908318dc39a4c73d",
            "3ce12ff63d674144860303ef63c2ea1f",
            "44270e0214f94455b941426edafdbfbf",
            "be1f469d877943da95a12920ffcf251c",
            "62c24d355f024dc8aa96a6cad9a73d46",
            "40fcdc64a9524ac78a09267a34bd0bb3",
            "4be8fd9593e3475d83f08ca04db38730",
            "c855d45c2ff4446c8b6defa38523c196",
            "9ad81f901786438ab794d660b8f13d07",
            "da0baa1ac386412ba9a8fe5818a08431",
            "7e9ee597009440f3809d6c43864fba36",
            "d4239fd96be14211bb93876df3a82fb4",
            "6cd3f5699fc848b6a904ec0e62981577",
            "76f44e5b1d434830aa4cccd42e7032f6",
            "f557982dc849418ea782f627dc80ed09",
            "10b70d004cb84d36aa91656df6d50d41",
            "4fc94aeddb124423af9181734fabac0a",
            "782c165486fe44f9b0fcd3bf09061c22",
            "b7545e601bd34db1a5b8bbce4d1fde2e",
            "b42ae97031dc42339dd41355c490e298",
            "9dfbc747eabc41659d1f3aec153af2f2",
            "c03a6b8552a24358838997d941729e8e",
            "a7d2ad55c98a4036a76c32154692d434",
            "5b039db23cda40f480f3ccba8067da77",
            "7b750dcc7bea43748671929c9a3ccef0",
            "786c0e7f4d3a4d4cae26e1ce93b4325d",
            "6c7b0f05ff904879a5df71e3bbe6f0ff",
            "335115c652ec4878b916e7d068b57653",
            "21befe4605d0475bbd56c996b46d5153",
            "20ad33ea943b46859a83a7c8dad770ac",
            "024f9598509d4510af71e7ecaf0b572e",
            "15cb6b56636246cf8af03e40d3c2f50b",
            "9f288008153e4d08b7e2768612096c9e",
            "b1be9b09adc544bc919cc336ed20521a",
            "6fc5d5adb55d4aaaab50713221a6bac3",
            "555ae10a003846c39d5c99e72d3c45ac",
            "d7ea3d7261b941eba6cfdae734c931ca",
            "3afe0fe1771947ecb3f0390db9e41dad",
            "1e1bfab56cc14255a985705f0543204b",
            "56f158e1c42546df9e8f6e3ccf6f1c34",
            "1a55e637cda04a74aca00b12dcbd3bc6"
          ]
        },
        "id": "PUDYTQ4vSD9M",
        "outputId": "d43698fe-62ba-4ba0-8b30-6f338267f302"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading google/functiongemma-270m-it...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ecc716276773474e9a1d3d068e84817b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/1.32k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "20402aaf49b74f7eb2fdb7416e23943a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/536M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dac398efc94a46f9a5e875d2dc3ccf73",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/176 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "efd8dfd28c504176a552fd66ea78aae7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6e156bdb62af4a2780bee18d670efad3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1751a001d19b49d6b256e3f656425803",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9ad81f901786438ab794d660b8f13d07",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/63.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b42ae97031dc42339dd41355c490e298",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/706 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "024f9598509d4510af71e7ecaf0b572e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "chat_template.jinja:   0%|          | 0.00/13.8k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Model loaded!\n",
            "   Parameters: 268,098,176\n",
            "   Memory: ~0.5 GB (bfloat16)\n",
            "   Device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "# Load FunctionGemma base model\n",
        "BASE_MODEL = \"google/functiongemma-270m-it\"\n",
        "\n",
        "print(f\"Loading {BASE_MODEL}...\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    torch_dtype=torch.bfloat16,      # 16-bit to save VRAM\n",
        "    device_map=\"auto\",                # Automatically load to GPU\n",
        "    attn_implementation=\"eager\"       # Without FlashAttention for compatibility\n",
        ")\n",
        "\n",
        "# Tokenizer converts text to tokens and back\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
        "\n",
        "print(f\"\\nModel loaded!\")\n",
        "print(f\"   Parameters: {model.num_parameters():,}\")\n",
        "print(f\"   Memory: ~{model.num_parameters() * 2 / 1e9:.1f} GB (bfloat16)\")\n",
        "print(f\"   Device: {model.device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vw7gBtGMSwJp"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Create SFTTrainer and start training\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset['train'],\n",
        "    eval_dataset=dataset['test'],\n",
        "    processing_class=tokenizer,  # TRL 0.26.2: use processing_class, not tokenizer\n",
        ")\n",
        "\n",
        "print(\"Starting training...\")\n",
        "print(f\"   Train examples: {len(dataset['train'])}\")\n",
        "print(f\"   Eval examples: {len(dataset['test'])}\")\n",
        "print(f\"   Format: Google FunctionGemma (manual)\")\n",
        "print(f\"   Estimated time: ~5 minutes on A100\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Train!\n",
        "train_result = trainer.train()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"Training complete!\")\n",
        "print(f\"   Final loss: {train_result.training_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWCay9_TSPsL",
        "outputId": "dba229f4-e193-4331-9209-be3b2d0e21c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training configuration (Google official params):\n",
            "   Epochs: 4\n",
            "   Batch size: 4\n",
            "   Gradient accumulation: 8\n",
            "   Effective batch size: 32\n",
            "   Learning rate: 5e-05\n",
            "   LR scheduler: SchedulerType.COSINE\n",
            "   Max length: 512\n",
            "   Dataset field: text\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from trl import SFTConfig, SFTTrainer\n",
        "\n",
        "# Output directory\n",
        "OUTPUT_DIR = \"functiongemma-mobile-demo\"\n",
        "\n",
        "# =============================================================================\n",
        "# Training configuration (based on official Google FunctionGemma cookbook)\n",
        "# https://github.com/google-gemini/gemma-cookbook/blob/main/FunctionGemma/\n",
        "# =============================================================================\n",
        "training_args = SFTConfig(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "\n",
        "    # Dataset field with pre-formatted Google FunctionGemma format\n",
        "    dataset_text_field=\"text\",          # Use our pre-formatted text, NOT apply_chat_template\n",
        "\n",
        "    # Training params (Google official uses 2 epochs, we use 5 for enum support)\n",
        "    max_length=512,                    # Max sequence length in tokens\n",
        "    packing=False,                      # Don't pack multiple examples into one sequence\n",
        "    num_train_epochs=4,                 # Extended training for enum support (320 examples)\n",
        "    per_device_train_batch_size=4,      # Batch size per GPU\n",
        "    per_device_eval_batch_size=4,       # Eval batch size\n",
        "    gradient_accumulation_steps=8,      # Effective batch size: 4 * 8 = 32\n",
        "\n",
        "    learning_rate=5e-5,                 # Google official: 1e-5 (more conservative than 5e-5)\n",
        "    lr_scheduler_type=\"cosine\",         # Google official: cosine decay\n",
        "    optim=\"adamw_torch_fused\",          # Fused AdamW for faster training\n",
        "    warmup_ratio=0.1,                   # 10% warmup steps\n",
        "\n",
        "    # Logging and checkpoints\n",
        "    logging_steps=10,                   # Log every 10 steps\n",
        "    eval_strategy=\"epoch\",              # Evaluate after each epoch\n",
        "    save_strategy=\"epoch\",              # Save checkpoint after each epoch\n",
        "\n",
        "    # Memory optimization\n",
        "    gradient_checkpointing=False,       # Trade compute for memory (enable if OOM)\n",
        "    bf16=True,                          # Use bfloat16 for training\n",
        "\n",
        "    # Output\n",
        "    report_to=\"tensorboard\",            # Log to TensorBoard\n",
        "    push_to_hub=False,                  # Set to True to upload to HuggingFace\n",
        ")\n",
        "\n",
        "print(\"Training configuration (Google official params):\")\n",
        "print(f\"   Epochs: {training_args.num_train_epochs}\")\n",
        "print(f\"   Batch size: {training_args.per_device_train_batch_size}\")\n",
        "print(f\"   Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
        "print(f\"   Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
        "print(f\"   Learning rate: {training_args.learning_rate}\")\n",
        "print(f\"   LR scheduler: {training_args.lr_scheduler_type}\")\n",
        "print(f\"   Max length: {training_args.max_length}\")\n",
        "print(f\"   Dataset field: {training_args.dataset_text_field}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 631,
          "referenced_widgets": [
            "f93717c9d36b408db46bb759cd4a809e",
            "3c4b5e8438184447be3ad358a31b43dd",
            "8d8b93cf772849a8a194ae5489adf811",
            "027f09d5c7c94ef7a7df71f712b7678a",
            "4a3c17ede6b74bd3ab359c64ea440c14",
            "b17f0d38de9948d5bf0c1c489c131c31",
            "2cee93a85b5d418295f1f558b916d879",
            "41b2fe5e26ff4b45a89f76bd10d3b635",
            "baa29feab0224796af80d9033d13957f",
            "c0f243a8230e4fb0a08ca01d57e87aee",
            "71e6dc70b7e942eea09ce06e0dc2e11e",
            "8a61c90c238f490cbe84eeb1cd5cc3e5",
            "4c6e93ba1b044f07b6d54e9c5cf38df4",
            "0eb57996524f480a94925746891575fc",
            "4446e609e08545b98a83ba84fb1bd7d4",
            "cd15e263d97a4d4f89bf2cf207f3ae73",
            "ac41d8a255994e3e881d90542438196d",
            "b5714203310144c590232cb6375e1f80",
            "1a5c82046d7f4e7dad9f12d32a6780c7",
            "6accd4b8e029424ca3630379ee2d0069",
            "eb5cc3f350f14f25bd4fe1acaaee21f2",
            "fc859a4f06e1490da5aebf766973158d",
            "c23a17d3ac48437ea61da584720dd1f4",
            "5f7e8a0622ad40648519eab9fe7505ed",
            "e7173ee892864cfa8c693fb3494b2cb0",
            "b762e274b37d4825aebfc8d6e56bfb8e",
            "6421078def90400595840652f8d7bb6c",
            "5f1d4b5a1a05455b8ce6e540c051f318",
            "8e42c282ebfe4150b878711018d93503",
            "f986b4aaa8a54da4b16cc407dd874a40",
            "dc587bb959db40729fc2f39ed4cd835a",
            "0ccaf3164d22441495abe50fe8c9a17d",
            "7ba35dc7166e490ba0329dff414bb521",
            "9c1a52a99dbc415098e6e15d754199d7",
            "8513660df70f40bebe68ad7aae6e5dfd",
            "8b770a957d6d4c11875134667219ef03",
            "2284d43034d74cb28414c9830357e289",
            "5814240a7e5b4bc69554613dd00ee88a",
            "b05fb6f441564d448a37edd2cda6da29",
            "e3c9c1f91c06407e9e7ed64a9758747e",
            "fb1bc497b3df4025b593febf2583064f",
            "0e5f8fec9c374bb78d7060079f72a0bc",
            "82b2bd3617484e3d8ee31c246902ac92",
            "2b309a6cd72f4e8d98dfef5e3eca6c25",
            "efe601c307ad4cb4a291cbde34677428",
            "b23e00d1673b408dac88568880130112",
            "53fe139fe92447c591d45eb123907a4e",
            "ae692afe78004e9b8a23beee54d19a65",
            "cf32d3a4d09c4f2c8b28daf37e81f35c",
            "20b521978ff6489287c688f03cf3118d",
            "a62ea91ec35c48c292726b80d91d9471",
            "191133e62c264457ad9a24c311375ef9",
            "82c671d7ecf0488e955103a42ad38a9e",
            "13b832161baf43eb990098b67094ffbb",
            "b256be008f2044d7ab6f69e8c2882e71",
            "508bfddc1a744df290404fbe4edfe833",
            "9052feb0b7244a7c80e8a4fc90ab51be",
            "c6822e1dd9394f3ba594cd31ec75695c",
            "737da0a0a15b4684be2c208c6366322f",
            "63079286836041f4814f83f0f2b687c0",
            "637274fde8f247c4bb4087716cf70699",
            "b4234754a3c54d09ad70d7373bf17069",
            "f5f4dd4e175849479de7135ec3c7898d",
            "1b5b2f1eb5e04a33b6ea85073e5247a5",
            "4e1e4171d4a242b283e288ece4ce5ad5",
            "ad07d0f5b3a84595a4ec1ef703a2545f"
          ]
        },
        "id": "y4gc5SJrTQZY",
        "outputId": "5fd37220-d72d-47e8-d495-2c884595ccd4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f93717c9d36b408db46bb759cd4a809e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Adding EOS to train dataset:   0%|          | 0/800 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8a61c90c238f490cbe84eeb1cd5cc3e5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing train dataset:   0%|          | 0/800 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c23a17d3ac48437ea61da584720dd1f4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Truncating train dataset:   0%|          | 0/800 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9c1a52a99dbc415098e6e15d754199d7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Adding EOS to eval dataset:   0%|          | 0/200 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "efe601c307ad4cb4a291cbde34677428",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing eval dataset:   0%|          | 0/200 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "508bfddc1a744df290404fbe4edfe833",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Truncating eval dataset:   0%|          | 0/200 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 1, 'bos_token_id': 2, 'pad_token_id': 0}.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training...\n",
            "   Train examples: 800\n",
            "   Eval examples: 200\n",
            "   Format: Google FunctionGemma (manual)\n",
            "   Estimated time: ~5 minutes on A100\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [100/100 11:09, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.197400</td>\n",
              "      <td>0.099596</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.092000</td>\n",
              "      <td>0.093686</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.086900</td>\n",
              "      <td>0.087731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.085100</td>\n",
              "      <td>0.087411</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "Training complete!\n",
            "   Final loss: 0.4809\n"
          ]
        }
      ],
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset['train'],\n",
        "    eval_dataset=dataset['test'],\n",
        "    processing_class=tokenizer,\n",
        ")\n",
        "\n",
        "print(\"Starting training...\")\n",
        "print(f\"   Train examples: {len(dataset['train'])}\")\n",
        "print(f\"   Eval examples: {len(dataset['test'])}\")\n",
        "print(f\"   Format: Google FunctionGemma (manual)\")\n",
        "print(f\"   Estimated time: ~5 minutes on A100\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Train!\n",
        "train_result = trainer.train()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"Training complete!\")\n",
        "print(f\"   Final loss: {train_result.training_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohkCkxKkTM8Y",
        "outputId": "c8d71df4-d1d5-4b0d-ea8e-7f7355d7b703"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Model saved locally to functiongemma-mobile-demo-final/\n",
            "\n",
            "Model copied to Google Drive: /content/drive/MyDrive/functiongemma-mobile-demo-final/\n",
            "You can now use this in the conversion notebook!\n",
            "total 561990\n",
            "-rw------- 1 root root        63 Jan 20 17:02 added_tokens.json\n",
            "-rw------- 1 root root     13792 Jan 20 17:02 chat_template.jinja\n",
            "-rw------- 1 root root      1341 Jan 20 17:02 config.json\n",
            "-rw------- 1 root root       225 Jan 20 17:02 generation_config.json\n",
            "-rw------- 1 root root 536223056 Jan 20 17:02 model.safetensors\n",
            "-rw------- 1 root root       706 Jan 20 17:02 special_tokens_map.json\n",
            "-rw------- 1 root root   1155714 Jan 20 17:02 tokenizer_config.json\n",
            "-rw------- 1 root root  33384899 Jan 20 17:02 tokenizer.json\n",
            "-rw------- 1 root root   4689144 Jan 20 17:02 tokenizer.model\n",
            "-rw------- 1 root root      6289 Jan 20 17:02 training_args.bin\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "FINAL_MODEL_DIR = f\"{OUTPUT_DIR}-final\"\n",
        "DRIVE_MODEL_DIR = f\"/content/drive/MyDrive/{FINAL_MODEL_DIR}\"\n",
        "\n",
        "# Save model weights and config\n",
        "trainer.save_model(FINAL_MODEL_DIR)\n",
        "\n",
        "# Save tokenizer (needed for inference)\n",
        "tokenizer.save_pretrained(FINAL_MODEL_DIR)\n",
        "\n",
        "print(f\"Model saved locally to {FINAL_MODEL_DIR}/\")\n",
        "\n",
        "# Copy to Google Drive\n",
        "!cp -r {FINAL_MODEL_DIR} /content/drive/MyDrive/\n",
        "\n",
        "print(f\"\\nModel copied to Google Drive: {DRIVE_MODEL_DIR}/\")\n",
        "print(\"You can now use this in the conversion notebook!\")\n",
        "!ls -la {DRIVE_MODEL_DIR}/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XiFWsiNiT1OR",
        "outputId": "892fc9ef-50cf-4d17-822c-375e7e2137bb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing fine-tuned model (PIX pagamentos):\n",
            "============================================================\n",
            "\n",
            "User: manda 10 no pix\n",
            "Model: call:pagamento{valor:10.0,metodo_pagamento:pix}<eos>\n",
            "   ✅ Correct format!\n",
            "------------------------------------------------------------\n",
            "\n",
            "User: faz um pix de 25 reais\n",
            "Model: call:pagamento{valor:25.0,metodo_pagamento:pix}<eos>\n",
            "   ✅ Correct format!\n",
            "------------------------------------------------------------\n",
            "\n",
            "User: envia 42 via pix\n",
            "Model: call:pagamento{valor:42.0,metodo_pagamento:pix}<eos>\n",
            "   ✅ Correct format!\n",
            "------------------------------------------------------------\n",
            "\n",
            "User: me faz um pix de 100\n",
            "Model: call:pagamento{valor:100.0,metodo_pagamento:pix}<eos>\n",
            "   ✅ Correct format!\n",
            "------------------------------------------------------------\n",
            "\n",
            "User: pode mandar 9.90 no pix?\n",
            "Model: call:pagamento{valor:9.90,metodo_pagamento:pix}<eos>\n",
            "   ✅ Correct format!\n",
            "------------------------------------------------------------\n",
            "\n",
            "User: pix 300\n",
            "Model: call:pagamento{valor:300.0,metodo_pagamento:pix}<eos>\n",
            "   ✅ Correct format!\n",
            "------------------------------------------------------------\n",
            "\n",
            "User: manda um pix rapidinho de 75\n",
            "Model: call:pagamento{valor:75.0,metodo_pagamento:pix}<eos>\n",
            "   ✅ Correct format!\n",
            "------------------------------------------------------------\n",
            "\n",
            "User: transfere 150 no pix\n",
            "Model: call:pagamento{valor:150.0,metodo_pagamento:pix}<eos>\n",
            "   ✅ Correct format!\n",
            "------------------------------------------------------------\n",
            "\n",
            "User: paga 20 via pix\n",
            "Model: call:pagamento{valor:20.0,metodo_pagamento:pix}<eos>\n",
            "   ✅ Correct format!\n",
            "------------------------------------------------------------\n",
            "\n",
            "User: joga 5 no pix\n",
            "Model: call:pagamento{valor:5.0,metodo_pagamento:pix}<eos>\n",
            "   ✅ Correct format!\n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# Test the fine-tuned model on new prompts\n",
        "# =============================================================================\n",
        "# CRITICAL: Use the same Google format as training (not apply_chat_template!)\n",
        "\n",
        "test_prompts = [\n",
        "    \"manda 10 no pix\",\n",
        "    \"faz um pix de 25 reais\",\n",
        "    \"envia 42 via pix\",\n",
        "    \"me faz um pix de 100\",\n",
        "    \"pode mandar 9.90 no pix?\",\n",
        "    \"pix 300\",\n",
        "    \"manda um pix rapidinho de 75\",\n",
        "    \"transfere 150 no pix\",\n",
        "    \"paga 20 via pix\",\n",
        "    \"joga 5 no pix\"\n",
        "]\n",
        "\n",
        "print(\"Testing fine-tuned model (PIX pagamentos):\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for prompt in test_prompts:\n",
        "    # Create prompt in SAME format as training (Google FunctionGemma)\n",
        "    input_text = f\"\"\"{SYSTEM_PROMPT}{START_TURN}user\n",
        "{prompt}\n",
        "{END_TURN}\n",
        "{START_TURN}model\n",
        "\"\"\"\n",
        "\n",
        "    # Tokenize and send to GPU\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # Generate response\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=100,\n",
        "        do_sample=False,\n",
        "        pad_token_id=tokenizer.pad_token_id\n",
        "    )\n",
        "\n",
        "    # Decode only new tokens (without prompt)\n",
        "    response = tokenizer.decode(\n",
        "        outputs[0][inputs['input_ids'].shape[1]:],\n",
        "        skip_special_tokens=False\n",
        "    )\n",
        "\n",
        "    print(f\"\\nUser: {prompt}\")\n",
        "    print(f\"Model: {response.strip()}\")\n",
        "\n",
        "    # Verify format: must be Google FunctionGemma call\n",
        "    if response.strip().startswith(\"call:pagamento\"):\n",
        "        print(\"   ✅ Correct format!\")\n",
        "    else:\n",
        "        print(\"   ⚠️  Unexpected format\")\n",
        "\n",
        "    print(\"-\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATBJJOk-Y_Wz"
      },
      "source": [
        "### Convert pytorch to gguf\n",
        "\n",
        "1) Clone o llama.cpp:\n",
        "\n",
        "`git clone https://github.com/ggerganov/llama.cpp`\n",
        "\n",
        "`cd llama.cpp`\n",
        "\n",
        "2) Instale as dependências:\n",
        "\n",
        "`pip install -r requirements.txt`\n",
        "\n",
        "\n",
        "3) Converter para GGUF\n",
        "\n",
        "`python convert_hf_to_gguf.py \\\n",
        "  path/do/modelo_hf \\\n",
        "  --outfile model.gguf`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nJqSC7q57lJ",
        "outputId": "022a8171-32cd-4ec7-b2fd-e2b6342303c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.3.16.tar.gz (50.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python) (4.15.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python) (1.26.4)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python) (3.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.3)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.16-cp312-cp312-linux_x86_64.whl size=4422289 sha256=0e1b44b47b8fbeb7e3db0bbb22beddc4686215090213860abd773ca1b11f345d\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/82/ab/8784ee3fb99ddb07fd36a679ddbe63122cc07718f6c1eb3be8\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.3.16\n"
          ]
        }
      ],
      "source": [
        "! pip install llama-cpp-python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bEImJ03A_3X"
      },
      "source": [
        "### FunctionGemma - Full Precision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQs55LgZ9v5n",
        "outputId": "5985a37e-46e3-48f9-8420-b3d774a44c43"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_context: n_ctx_per_seq (512) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'role': 'assistant', 'content': 'call:pagamento{valor:25.0,metodo_pagamento:pix}<end_function_call>'}\n"
          ]
        }
      ],
      "source": [
        "from llama_cpp import Llama\n",
        "import json\n",
        "\n",
        "llm = Llama(\n",
        "    model_path=\"/content/sample_data/llama.cpp/functiongemma-f16.gguf\",\n",
        "    n_ctx=512,\n",
        "    verbose=False\n",
        "\n",
        ")\n",
        "\n",
        "response = llm.create_chat_completion(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"faz um pix de 25 reais\"\n",
        "        }\n",
        "    ],\n",
        "    tools=TOOLS,\n",
        "    tool_choice=\"auto\",\n",
        ")\n",
        "\n",
        "choice = response[\"choices\"][0][\"message\"]\n",
        "print(choice)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dPdonzsE3XW"
      },
      "outputs": [],
      "source": [
        "dataset = [\n",
        "    \"faz um pix de 10 reais\",\n",
        "    \"faz um pix de 25 reais\",\n",
        "    \"faz um pix de 100 reais\",\n",
        "    \"manda um pix de 5 reais\",\n",
        "    \"envia um pix de 50 reais\",\n",
        "    \"realiza um pix de 30 reais\",\n",
        "    \"paga 20 reais no pix\",\n",
        "    \"pix de 15 reais\",\n",
        "    \"pix 40 reais\",\n",
        "    \"transferir 60 reais via pix\",\n",
        "\n",
        "    \"pode fazer um pix de 25 pra mim?\",\n",
        "    \"faz um pix de 10 aí\",\n",
        "    \"manda 50 no pix\",\n",
        "    \"consegue mandar um pix de 20?\",\n",
        "    \"faz um pix pra mim de 35 reais\",\n",
        "    \"queria fazer um pix de 12 reais\",\n",
        "    \"dá pra transferir 80 via pix?\",\n",
        "    \"me ajuda a fazer um pix de 5 reais\",\n",
        "    \"vou pagar 70 reais no pix\",\n",
        "    \"preciso fazer um pix de 100\",\n",
        "\n",
        "    \"faz um pix de vinte reais\",\n",
        "    \"manda um pix de cinquenta reais\",\n",
        "    \"pix de cem reais\",\n",
        "    \"pagar trinta reais via pix\",\n",
        "    \"envia dez reais no pix\",\n",
        "\n",
        "    \"faz um pix de 25 reais pra pagar isso\",\n",
        "    \"manda 40 reais no pix pra ele\",\n",
        "    \"preciso pagar 60 reais no pix\",\n",
        "    \"faz um pix de 90 pra quitar\",\n",
        "    \"pix de 15 reais pra hoje\",\n",
        "\n",
        "    \"25 reais no pix\",\n",
        "    \"no pix, manda 50 reais\",\n",
        "    \"via pix, pagar 30 reais\",\n",
        "    \"10 reais via pix\",\n",
        "    \"pix no valor de 70 reais\",\n",
        "\n",
        "    \"pix 25\",\n",
        "    \"mandar pix 10\",\n",
        "    \"faz pix 50 reais\",\n",
        "    \"pagamento pix 40\",\n",
        "    \"transfer pix 60\",\n",
        "\n",
        "    \"faz um pix de R$25\",\n",
        "    \"manda um pix de R$ 10,00\",\n",
        "    \"pix de 20 conto\",\n",
        "    \"manda vinte no pix\",\n",
        "    \"paga 15 no pix\"\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHeKxbLrKBir",
        "outputId": "42066e8c-7d0a-4c23-f22a-f7ce5ff1c47a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "45"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opafre0oCGUl",
        "outputId": "bbf0951e-28a3-44a1-be9f-2e9ec3ff66e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'role': 'assistant', 'content': 'call:pagamento{valor:10.0,metodo_pagamento:pix}<end_function_call>'}\n",
            "{'role': 'assistant', 'content': 'call:pagamento{valor:25.0,metodo_pagamento:pix}<end_function_call>'}\n",
            "{'role': 'assistant', 'content': 'call:pagamento{valor:100.0,metodo_pagamento:pix}<end_function_call>'}\n",
            "{'role': 'assistant', 'content': 'call:pagamento{valor:5.0,metodo_pagamento:pix}<end_function_call>'}\n",
            "{'role': 'assistant', 'content': 'call:pagamento{valor:50.0,metodo_pagamento:pix}<end_function_call>'}\n",
            "{'role': 'assistant', 'content': 'call:pagamento{valor:30.0,metodo_pagamento:pix}<end_function_call>'}\n",
            "{'role': 'assistant', 'content': 'call:pagamento{valor:20.0,metodo_pagamento:pix}<end_function_call>'}\n",
            "{'role': 'assistant', 'content': 'call:pagamento{valor:15.0,metodo_pagamento:pix}<end_function_call>'}\n",
            "{'role': 'assistant', 'content': 'call:pagamento{valor:40.0,metodo_pagamento:pix}<end_function_call>'}\n",
            "{'role': 'assistant', 'content': 'call:pagamento{valor:60.0,metodo_pagamento:pix}<end_function_call>'}\n",
            "{'role': 'assistant', 'content': 'call:pagamento{valor:25.0,metodo_pagamento:pix}<end_function_call>'}\n",
            "{'role': 'assistant', 'content': 'call:pagamento{valor:10.0,metodo_pagamento:pix}<end_function_call>'}\n",
            "{'role': 'assistant', 'content': 'call:pagamento{valor:50.0,metodo_pagamento:pix}<end_function_call>'}\n",
            "{'role': 'assistant', 'content': 'call:pagamento{valor:20.0,metodo_pagamento:pix}<end_function_call>'}\n",
            "{'role': 'assistant', 'content': 'call:pagamento{valor:35.0,metodo_pagamento:pix}<end_function_call>'}\n",
            "{'role': 'assistant', 'content': 'call:pagamento{valor:12.0,metodo_pagamento:pix}<end_function_call>'}\n",
            "{'role': 'assistant', 'content': 'call:pagamento{valor:80.0,metodo_pagamento:pix}<end_function_call>'}\n",
            "{'role': 'assistant', 'content': 'call:pagamento{valor:5.0,metodo_pagamento:pix}<end_function_call>'}\n",
            "{'role': 'assistant', 'content': 'call:pagamento{valor:70.0,metodo_pagamento:pix}<end_function_call>'}\n",
            "{'role': 'assistant', 'content': 'call:pagamento{valor:100.0,metodo_pagamento:pix}<end_function_call>'}\n",
            "{'role': 'assistant', 'content': 'call:pagamento{valor:30.0,metodo_pagamento:pix}<end_function_call>'}\n",
            "{'role': 'assistant', 'content': 'call:pagamento{valor:50.99,metodo_pagamento:pix}<end_function_call>'}\n",
            "{'role': 'assistant', 'content': 'call:pagamento{valor:39.99,metodo_pagamento:pix}<end_function_call>'}\n",
            "{'role': 'assistant', 'content': 'call:pagamento{valor:30.0,metodo_pagamento:pix}<end_function_call>'}\n",
            "{'role': 'assistant', 'content': 'call:pagamento{valor:10.0,metodo_pagamento:pix}<end_function_call>'}\n",
            "{'role': 'assistant', 'content': 'call:pagamento{valor:25.0,metodo_pagamento:pix}<end_function_call>'}\n",
            "{'role': 'assistant', 'content': 'call:pagamento{valor:40.0,metodo_pagamento:pix}<end_function_call>'}\n",
            "{'role': 'assistant', 'content': 'call:pagamento{valor:60.0,metodo_pagamento:pix}<end_function_call>'}\n",
            "{'role': 'assistant', 'content': 'call:pagamento{valor:90.0,metodo_pagamento:pix}<end_function_call>'}\n",
            "{'role': 'assistant', 'content': 'call:pagamento{valor:15.0,metodo_pagamento:pix}<end_function_call>'}\n",
            "{'role': 'assistant', 'content': 'call:pagamento{valor:25.0,metodo_pagamento:pix}<end_function_call>'}\n",
            "{'role': 'assistant', 'content': 'call:pagamento{valor:50.0,metodo_pagamento:pix}<end_function_call>'}\n",
            "{'role': 'assistant', 'content': 'call:pagamento{valor:30.0,metodo_pagamento:pix}<end_function_call>'}\n",
            "{'role': 'assistant', 'content': 'call:pagamento{valor:10.0,metodo_pagamento:pix}<end_function_call>'}\n",
            "{'role': 'assistant', 'content': 'call:pagamento{valor:70.0,metodo_pagamento:pix}<end_function_call>'}\n",
            "{'role': 'assistant', 'content': 'call:pagamento{valor:25.0,metodo_pagamento:pix}<end_function_call>'}\n",
            "{'role': 'assistant', 'content': 'call:pagamento{valor:10.0,metodo_pagamento:pix}<end_function_call>'}\n",
            "{'role': 'assistant', 'content': 'call:pagamento{valor:50.0,metodo_pagamento:pix}<end_function_call>'}\n",
            "{'role': 'assistant', 'content': 'call:pagamento{valor:40.0,metodo_pagamento:pix}<end_function_call>'}\n",
            "{'role': 'assistant', 'content': 'call:pagamento{valor:60.0,metodo_pagamento:pix}<end_function_call>'}\n",
            "{'role': 'assistant', 'content': 'call:pagamento{valor:25.0,metodo_pagamento:pix}<end_function_call>'}\n",
            "{'role': 'assistant', 'content': 'call:pagamento{valor:1000.0,metodo_pagamento:pix}<end_function_call>'}\n",
            "{'role': 'assistant', 'content': 'call:pagamento{valor:20.0,metodo_pagamento:pix}<end_function_call>'}\n",
            "{'role': 'assistant', 'content': 'call:pagamento{valor:30.0,metodo_pagamento:pix}<end_function_call>'}\n",
            "{'role': 'assistant', 'content': 'call:pagamento{valor:15.0,metodo_pagamento:pix}<end_function_call>'}\n"
          ]
        }
      ],
      "source": [
        "for falas in dataset:\n",
        "    response = llm.create_chat_completion(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": falas\n",
        "            }\n",
        "        ],\n",
        "        tools=TOOLS,\n",
        "        tool_choice=\"auto\",\n",
        "    )\n",
        "\n",
        "    message = response[\"choices\"][0][\"message\"]\n",
        "    print(message)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enXbg3o_IqGC"
      },
      "source": [
        "Chamada de função: 45/45 dos casos\n",
        "\n",
        "Chamada correta de função e parâmetros: 40/45"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96T5Z9W9Xw3r"
      },
      "outputs": [],
      "source": [
        "# convert pytorch -> gguf q2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lg_obNtfYkg6"
      },
      "outputs": [],
      "source": [
        "# convert pytorch -> gguf q3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIFpfecHYmto"
      },
      "outputs": [],
      "source": [
        "# convert pytorch -> gguf q4"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPLhrLBUW7H+7WqDkn1sD9J",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}